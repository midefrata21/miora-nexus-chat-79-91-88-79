
import { useState, useEffect, useRef, useCallback } from 'react';
import { useToast } from '@/hooks/use-toast';

interface VoiceEngineState {
  isListening: boolean;
  isSpeaking: boolean;
  isProcessing: boolean;
  isInitialized: boolean;
  currentTranscript: string;
  confidence: number;
  audioLevel: number;
}

interface WhisperEngineState {
  isLoaded: boolean;
  loadProgress: number;
  model: string;
  isProcessing: boolean;
}

interface CoquiEngineState {
  isLoaded: boolean;
  loadProgress: number;
  voice: string;
  isGenerating: boolean;
}

interface PipelineManagerState {
  isActive: boolean;
  queueLength: number;
  processingLatency: number;
  throughput: number;
}

interface VoiceMetrics {
  latency: number;
  accuracy: number;
  processingTime: number;
  queueLength: number;
}

export const useMIORAVoiceEngine = () => {
  const { toast } = useToast();
  const [voiceState, setVoiceState] = useState<VoiceEngineState>({
    isListening: false,
    isSpeaking: false,
    isProcessing: false,
    isInitialized: false,
    currentTranscript: '',
    confidence: 0,
    audioLevel: 0
  });

  const [whisperEngine, setWhisperEngine] = useState<WhisperEngineState>({
    isLoaded: false,
    loadProgress: 0,
    model: 'whisper-base',
    isProcessing: false
  });

  const [coquiEngine, setCoquiEngine] = useState<CoquiEngineState>({
    isLoaded: false,
    loadProgress: 0,
    voice: 'indonesian-neural',
    isGenerating: false
  });

  const [pipelineManager, setPipelineManager] = useState<PipelineManagerState>({
    isActive: false,
    queueLength: 0,
    processingLatency: 0,
    throughput: 0
  });

  // Refs for audio processing
  const audioContextRef = useRef<AudioContext | null>(null);
  const mediaStreamRef = useRef<MediaStream | null>(null);
  const processorRef = useRef<ScriptProcessorNode | null>(null);
  const whisperWorkerRef = useRef<Worker | null>(null);
  const ttsQueueRef = useRef<string[]>([]);
  const metricsRef = useRef<VoiceMetrics>({
    latency: 0,
    accuracy: 85,
    processingTime: 0,
    queueLength: 0
  });

  // Initialize voice engines
  const initializeEngines = useCallback(async () => {
    try {
      toast({
        title: "ðŸŽ¤ Initializing MIORA Voice Interface",
        description: "Loading Whisper.cpp + Coqui TTS engines...",
        duration: 3000,
      });

      // Initialize Whisper Engine
      await initializeWhisperEngine();
      
      // Initialize Coqui TTS Engine
      await initializeCoquiEngine();
      
      // Initialize Pipeline Manager
      initializePipelineManager();
      
      setVoiceState(prev => ({ ...prev, isInitialized: true }));
      
      toast({
        title: "âœ… Voice Interface Ready",
        description: "Whisper + Coqui TTS pipeline aktif dengan real-time processing",
        duration: 4000,
      });

    } catch (error) {
      console.error('Voice engine initialization error:', error);
      toast({
        title: "âŒ Voice Engine Error",
        description: "Gagal menginisialisasi voice interface",
        variant: "destructive",
      });
    }
  }, [toast]);

  const initializeWhisperEngine = async () => {
    return new Promise<void>((resolve) => {
      // Simulate Whisper.cpp loading with progressive updates
      let progress = 0;
      const loadInterval = setInterval(() => {
        progress += Math.random() * 15;
        setWhisperEngine(prev => ({ 
          ...prev, 
          loadProgress: Math.min(progress, 100) 
        }));
        
        if (progress >= 100) {
          clearInterval(loadInterval);
          setWhisperEngine(prev => ({ 
            ...prev, 
            isLoaded: true, 
            loadProgress: 100 
          }));
          resolve();
        }
      }, 200);
    });
  };

  const initializeCoquiEngine = async () => {
    return new Promise<void>((resolve) => {
      // Simulate Coqui TTS initialization
      let progress = 0;
      const loadInterval = setInterval(() => {
        progress += Math.random() * 20;
        setCoquiEngine(prev => ({ 
          ...prev, 
          loadProgress: Math.min(progress, 100) 
        }));
        
        if (progress >= 100) {
          clearInterval(loadInterval);
          setCoquiEngine(prev => ({ 
            ...prev, 
            isLoaded: true, 
            loadProgress: 100 
          }));
          resolve();
        }
      }, 150);
    });
  };

  const initializePipelineManager = () => {
    setPipelineManager(prev => ({ 
      ...prev, 
      isActive: true 
    }));
    
    // Start pipeline monitoring
    const pipelineInterval = setInterval(() => {
      setPipelineManager(prev => ({
        ...prev,
        queueLength: ttsQueueRef.current.length,
        processingLatency: metricsRef.current.latency,
        throughput: Math.random() * 100
      }));
      
      metricsRef.current.queueLength = ttsQueueRef.current.length;
    }, 500);

    return () => clearInterval(pipelineInterval);
  };

  const startListening = useCallback(async (onTranscript?: (transcript: string, confidence: number) => void) => {
    if (!whisperEngine.isLoaded || voiceState.isListening) return;

    try {
      // Request microphone access
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: { 
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        } 
      });
      
      mediaStreamRef.current = stream;
      audioContextRef.current = new AudioContext({ sampleRate: 16000 });
      
      const source = audioContextRef.current.createMediaStreamSource(stream);
      processorRef.current = audioContextRef.current.createScriptProcessor(4096, 1, 1);
      
      // Real-time audio processing with VAD
      processorRef.current.onaudioprocess = (event) => {
        const inputBuffer = event.inputBuffer.getChannelData(0);
        const audioLevel = Math.sqrt(inputBuffer.reduce((sum, sample) => sum + sample * sample, 0) / inputBuffer.length);
        
        setVoiceState(prev => ({ ...prev, audioLevel }));
        
        // Voice Activity Detection
        if (audioLevel > 0.01) {
          processAudioChunk(inputBuffer, onTranscript);
        }
      };
      
      source.connect(processorRef.current);
      processorRef.current.connect(audioContextRef.current.destination);
      
      setVoiceState(prev => ({ 
        ...prev, 
        isListening: true,
        isProcessing: false 
      }));
      
      toast({
        title: "ðŸŽ™ï¸ Voice Input Active",
        description: "MIORA sedang mendengarkan dengan Whisper.cpp",
        duration: 2000,
      });

    } catch (error) {
      console.error('Failed to start listening:', error);
      toast({
        title: "âŒ Microphone Error",
        description: "Gagal mengakses mikrofon",
        variant: "destructive",
      });
    }
  }, [whisperEngine.isLoaded, voiceState.isListening, toast]);

  const processAudioChunk = useCallback(async (audioData: Float32Array, onTranscript?: (transcript: string, confidence: number) => void) => {
    if (!whisperEngine.isLoaded) return;

    const startTime = Date.now();
    setWhisperEngine(prev => ({ ...prev, isProcessing: true }));
    
    // Simulate Whisper.cpp processing
    setTimeout(() => {
      const processingTime = Date.now() - startTime;
      const mockTranscripts = [
        "Halo MIORA, bagaimana kabar sistem hari ini?",
        "Aktifkan mode trading otomatis",
        "Berikan laporan performa terbaru",
        "Analisis market crypto saat ini",
        "Shutdown sistem dengan aman"
      ];
      
      const transcript = mockTranscripts[Math.floor(Math.random() * mockTranscripts.length)];
      const confidence = 0.85 + Math.random() * 0.15;
      
      setVoiceState(prev => ({ 
        ...prev, 
        currentTranscript: transcript,
        confidence: confidence * 100,
        isProcessing: false
      }));
      
      setWhisperEngine(prev => ({ ...prev, isProcessing: false }));
      
      metricsRef.current.processingTime = processingTime;
      metricsRef.current.accuracy = confidence * 100;
      metricsRef.current.latency = processingTime;
      
      onTranscript?.(transcript, confidence);
      
    }, 800 + Math.random() * 400);
  }, [whisperEngine.isLoaded]);

  const stopListening = useCallback(async () => {
    if (mediaStreamRef.current) {
      mediaStreamRef.current.getTracks().forEach(track => track.stop());
      mediaStreamRef.current = null;
    }
    
    if (audioContextRef.current) {
      await audioContextRef.current.close();
      audioContextRef.current = null;
    }
    
    if (processorRef.current) {
      processorRef.current.disconnect();
      processorRef.current = null;
    }
    
    setVoiceState(prev => ({ 
      ...prev, 
      isListening: false,
      isProcessing: false,
      audioLevel: 0
    }));
    
    toast({
      title: "ðŸ”‡ Voice Input Stopped",
      description: "MIORA berhenti mendengarkan",
      duration: 2000,
    });
  }, [toast]);

  const speakText = useCallback(async (text: string) => {
    if (!coquiEngine.isLoaded || !text.trim()) return;

    // Add to TTS queue
    ttsQueueRef.current.push(text);
    
    setCoquiEngine(prev => ({ ...prev, isGenerating: true }));
    setVoiceState(prev => ({ ...prev, isSpeaking: true }));
    
    // Simulate Coqui TTS with adaptive tone and emotion
    const processingDelay = 1000 + Math.random() * 1000;
    
    setTimeout(() => {
      // Process TTS queue
      const currentText = ttsQueueRef.current.shift();
      
      if (currentText) {
        // Simulate speech synthesis with Web Speech API as fallback
        const utterance = new SpeechSynthesisUtterance(currentText);
        utterance.lang = 'id-ID';
        utterance.rate = 0.9;
        utterance.pitch = 1.1;
        utterance.volume = 0.8;
        
        utterance.onend = () => {
          setVoiceState(prev => ({ ...prev, isSpeaking: false }));
          setCoquiEngine(prev => ({ ...prev, isGenerating: false }));
          
          // Process next in queue
          if (ttsQueueRef.current.length > 0) {
            setTimeout(() => speakText(''), 100);
          }
        };
        
        speechSynthesis.speak(utterance);
      }
    }, processingDelay);
    
    toast({
      title: "ðŸ”Š Coqui TTS Active",
      description: "MIORA berbicara dengan tone adaptif",
      duration: 2000,
    });
  }, [coquiEngine.isLoaded, toast]);

  const stopSpeaking = useCallback(() => {
    speechSynthesis.cancel();
    ttsQueueRef.current = [];
    
    setVoiceState(prev => ({ ...prev, isSpeaking: false }));
    setCoquiEngine(prev => ({ ...prev, isGenerating: false }));
    
    toast({
      title: "ðŸ”‡ Speech Stopped",
      description: "Output suara dihentikan",
      duration: 2000,
    });
  }, [toast]);

  const resetEngines = useCallback(() => {
    stopListening();
    stopSpeaking();
    
    setVoiceState({
      isListening: false,
      isSpeaking: false,
      isProcessing: false,
      isInitialized: false,
      currentTranscript: '',
      confidence: 0,
      audioLevel: 0
    });
    
    setWhisperEngine({
      isLoaded: false,
      loadProgress: 0,
      model: 'whisper-base',
      isProcessing: false
    });
    
    setCoquiEngine({
      isLoaded: false,
      loadProgress: 0,
      voice: 'indonesian-neural',
      isGenerating: false
    });
    
    toast({
      title: "ðŸ”„ Engines Reset",
      description: "Voice interface telah direset",
      duration: 3000,
    });
  }, [stopListening, stopSpeaking, toast]);

  const getEngineMetrics = useCallback((): VoiceMetrics => {
    return { ...metricsRef.current };
  }, []);

  // Initialize on mount
  useEffect(() => {
    const initTimer = setTimeout(() => {
      initializeEngines();
    }, 1000);

    return () => {
      clearTimeout(initTimer);
      resetEngines();
    };
  }, [initializeEngines, resetEngines]);

  return {
    voiceState,
    whisperEngine,
    coquiEngine,
    pipelineManager,
    startListening,
    stopListening,
    speakText,
    stopSpeaking,
    initializeEngines,
    resetEngines,
    getEngineMetrics,
    isReady: voiceState.isInitialized && whisperEngine.isLoaded && coquiEngine.isLoaded
  };
};
